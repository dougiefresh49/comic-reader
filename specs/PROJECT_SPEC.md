# **Project: AI Comic Book Reader (Automated Pipeline)**

## **1\. Overview**

We are building a Node.js pipeline to convert static comic book images into an interactive, voice-acted web experience. The pipeline uses AI to identify speech bubbles, generate character-specific audio, and optimize assets for the web.

## **2\. Architecture & Directory Structure**

### **Root Structure**

```bash
/workspace
  /assets
    /comics
      /[book-id]
        /issue-[x]
          /pages
            /page-[x].jpg  \<-- INPUT: Source 2MB JPEGs (NOT public)
  /public
    /comics/[book-id]/issue-[x]/ \<-- OUTPUT (Phase 1): Optimized WebP \+ MP3s serve from here
  /scripts               \<-- The 4 Node.js processing scripts
  /data
    source-material.json \<-- Config: Character Name \-\> YouTube URL
    castList.js          \<-- Config: Character Name \-\> ElevenLabs Voice ID
    context-cache.json   \<-- Data: AI analysis of pages (Intermediate)
    fixes.json           \<-- Data: Human corrections from the review app
```

### **Data Flow (Local vs. Cloud)**

- **Phase 1 (Development):** Images and Audio are saved directly to /public/comics/\[book-id\]/. The manifest uses relative paths.
- **Phase 2 (Production):** Images and Audio are uploaded to S3/Blob Storage. The manifest uses absolute URLs.

## **3\. The 4-Script Pipeline**

### **Script 1: npm run get-context (Discovery)**

- **Goal:** Identify every speech bubble, who is speaking, and the emotion.
- **Input:** Raw images from /assets, Roboflow API (Location), Gemini 1.5 Flash API (Context).
- **Logic:**
  1. **Concurrency:** Use p-limit (limit: 3\) to handle requests without rate-limiting.
  2. **Detection:** Send page to Roboflow Rapid API \-\> Get Bounding Boxes.
  3. **OCR:** Crop boxes \-\> Send to Gemini 2.5 Flash \-\> Get Text.
  4. **Context:** Send Page \+ Text \+ Box to Gemini \-\> Get { Speaker, Emotion, Type }.
  5. **Rules:**
     - If Type is SPEECH \-\> Identify Speaker \+ Emotion.
     - If Type is NARRATION or CAPTION \-\> Force Speaker="Narrator", Emotion="Neutral".
     - If Type is SFX \-\> Ignore/Skip.
- **Output:** Saves results to /data/context-cache.json. Prints list of "New Characters" found.

### **Script 2: npm run create-voices (Audio Factory)**

- **Goal:** Create voice clones for characters found in Script 1\.
- **Input:** /data/source-material.json (Manual YouTube links).
  1. _Note:_ Includes "Narrator" mapped to the Cate Blanchett YouTube link.
- **Logic:**
  1. **Download:** Use youtube-dl-exec to fetch audio from links.
  2. **Pruning (Pause):** Script pauses and prompts user to edit raw files (using ElevenLabs Scribe or Descript) to remove unwanted speakers. User saves cleaned files to a specific folder.
  3. **Clean:** Upload pruned audio to ElevenLabs Voice Isolator.
  4. **Train:** Upload clean audio to ElevenLabs PVC (Professional Voice Cloning).
  5. **Minor Characters:** For characters _not_ in source-material.json (labeled "MINOR" by Gemini), use ElevenLabs **Voice Design API** to generate a voice instantly.
- **Output:** Updates /data/castList.js with new Voice IDs.

### **Script 3: npm run build-comic (Generation)**

- **Goal:** Create the playable assets and final manifest.
- **Input:** context-cache.json \+ castList.js \+ Raw Images.
- **Env Var:** STORAGE_MODE (Default: 'local', Options: 'local', 's3').
- **Logic:**
  1. **Images:** Use sharp to resize raw pages to 1200px width and convert to **WebP**.
     - If Local: Save to /public.
     - If S3: Upload to bucket.
  2. **Audio:** Loop through cache. Match Speaker \-\> Voice ID. Call ElevenLabs TTS API.
     - If Local: Save to /public.
     - If S3: Upload to bucket.
  3. **Manifest:** Generate the final manifest.json linking images to audio/timestamps (using correct relative or absolute paths based on storage mode).

### **Script 4: npm run apply-fixes (Maintenance)**

- **Goal:** Fix mistakes found during human review.
- **Input:** fixes.json (exported from the web app).
- **Logic:** Re-runs TTS _only_ for specific bubble IDs and updates the existing manifest.json surgically.

## **4\. Tech Stack & Requirements**

- **Runtime:** Node.js (ES Modules)
- **System Dependencies:** ffmpeg (Required for audio processing)
- **Key Libraries:**
  - fs-extra, path (File ops)
  - sharp (Image processing)
  - @google/genai (Gemini SDK v3)
  - elevenlabs-node (Voice)
  - youtube-dl-exec (Downloads)
  - p-limit (Concurrency control)

## **5\. The Reader Web App (Next.js)**

### **Goals**

A "dumb" frontend that focuses purely on UX, responsiveness, and playing the assets generated by the backend.

### **Architecture**

- **Framework:** Next.js (App Router).
- **Styling:** Tailwind CSS.
- **State Management:** React Context (for Audio playback state).

### **Key Routes**

1. **/comic/\[bookId\] (Public Reader)**
   - **View:** Full-screen comic page.
   - **Navigation:** Swipe/Arrow keys to change pages.
   - **Bubble Logic:**
     - **State:** Default is **Blank** (Hidden Text).
     - **Interaction:** User taps bubble.
     - **Action:** Play audio from audioSrc.
     - **Visual:** Reveal text. Use timestamps from manifest to highlight words in sync with audio (Karaoke effect).
2. **/comic/\[bookId\]/review (Admin Reviewer)**
   - **View:** Same comic view, but Bubbles are always visible (translucent).
   - **Interaction:** Tap bubble to open **Editor Modal**.
   - **Editor Modal:**
     - Dropdown: Change Speaker.
     - Dropdown: Change Emotion.
     - Button: "Mark for Redo".
   - **Export:** Floating "Save Fixes" button generates and downloads fixes.json to your computer.

### **Core Components**

- **\<ComicPage /\>**: Renders the WebP image (responsive).
- **\<InteractiveBubble /\>**:
  - Positioned absolute based on % coordinates (responsive).
  - Handles onClick to trigger audio.
  - Handles onTimeUpdate from audio to drive text highlighting.
- **\<AudioController /\>**: Global hook/context to ensure only one audio plays at a time.
